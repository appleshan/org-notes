#+TITLE: 分布式系统的CAP理论

@See http://www.hollischuang.com/archives/666

2000 年 7 月，加州大学伯克利分校的 Eric Brewer 教授在 ACM PODC 会议上提出 CAP 猜想。2年后，麻省理工学
院的 Seth Gilbert 和 Nancy Lynch 从理论上证明了 CAP。之后，CAP 理论正式成为分布式计算领域的公认定理。

无论你是一个系统架构师，还是一个普通开发，当你开发或者设计一个分布式系统的时候，CAP 理论是无论如何也绕不过去
的。本文就来介绍一下到底什么是 CAP 理论，如何证明 CAP 理论，以及 CAP 的权衡问题。

* CAP 理论概述
CAP 理论：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性
（Partition tolerance）这三项中的两项。
[[file:../../images/Teorema-CAP-2.png]]

读者需要注意的的是，CAP 理论中的 CA 和数据库事务中 ACID 的 CA 并完全是同一回事儿。两者之中的 A 都是 C
都是一致性(Consistency)。CAP 中的 A 指的是可用性（Availability），而 ACID 中的 A 指的是原子性
（Atomicity)，切勿混为一谈。

* CAP 的定义
** Consistency 一致性
一致性指 =“all nodes see the same data at the same time”= ，即更新操作成功并返回客户端完成后，
所有节点在同一时间的数据完全一致，所以，一致性，说的就是数据一致性。

对于一致性，可以分为从客户端和服务端两个不同的视角。
 - 从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。
 - 从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。

一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。

从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。

三种一致性策略：
 - 强一致性
对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。
 - 弱一致性
如果能容忍后续的部分或者全部访问不到，则是弱一致性。
 - 最终一致性
如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。

CAP 中说，不可能同时满足的这个一致性指的是强一致性。

** Availability 可用性
可用性指 =“Reads and writes always succeed”= ，即服务一直可用，而且是正常响应时间。

对于一个可用性的分布式系统，每一个非故障的节点必须对每一个请求作出响应。所以，一般我们在衡量一个系统的
可用性的时候，都是通过停机时间来计算的。

| 可用性分类 | 可用水平（%） | 年可容忍停机时间 |
-------------------------------------------
| 容错可用性                   | 99.9999 | <1 min    |
| 极高可用性                   |  99.999 | <5 min    |
| 具有故障自动恢复能力的可用性 |   99.99 | <53 min   |
| 高可用性                     |    99.9 | <8.8h     |
| 商品可用性                   |      99 | <43.8 min |

通常我们描述一个系统的可用性时，我们说淘宝的系统可用性可以达到 5 个 9，意思就是说他的可用水平是
99.999%，即全年停机时间不超过 =(1-0.99999)*365*24*60 = 5.256 min= ，这是一个极高的要求。

好的可用性主要是指系统能够很好的为用户服务，不出现用户操作失败或者访问超时等用户体验不好的情况。一个分布
式系统，上下游设计很多系统如负载均衡、WEB 服务器、应用代码、数据库服务器等，任何一个节点的不稳定都可以
影响可用性。

** Partition Tolerance 分区容错性
分区容错性指 =“the system continues to operate despite arbitrary message loss or failure
of part of the system”= ，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和
可用性的服务。

分区容错性和扩展性紧密相关。在分布式应用中，可能因为一些分布式的原因导致系统无法正常运转。好的分区容错性
要求能够使应用虽然是一个分布式系统，而看上去却好像是在一个可以运转正常的整体。比如现在的分布式系统中有某
一个或者几个机器宕掉了，其他剩下的机器还能够正常运转满足系统需求，或者是机器之间有网络异常，将分布式系统
分隔未独立的几个部分，各个部分还能维持分布式系统的运作，这样就具有好的分区容错性。

简单点说，就是在网络中断，消息丢失的情况下，系统如果还能正常工作，就是有比较好的分区容错性。

* CAP 的证明

[[file:../../images/intro-CAP_Proof-of-CAP_01.png]]
如上图，是我们证明 CAP 的基本场景，网络中有两个节点 N1 和 N2，可以简单的理解 N1 和 N2 分别是两台计算机，
他们之间网络可以连通，N1 中有一个应用程序 A，和一个数据库 V，N2 也有一个应用程序 B2 和一个数据库 V。现在，
A 和 B 是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。

在满足一致性的时候，N1 和 N2 中的数据是一样的，V0=V0。在满足可用性的时候，用户不管是请求 N1 或者 N2，都
会得到立即响应。在满足分区容错性的情况下，N1 和 N2 有任何一方宕机，或者网络不通的时候，都不会影响 N1 和 N2
彼此之间的正常运作。

[[file:../../images/intro-CAP_Proof-of-CAP_02.png]]
如上图，是分布式系统正常运转的流程，用户向 N1 机器请求数据更新，程序 A 更新数据库 Vo 为 V1，分布式系统将
数据进行同步操作 M，将 V1 同步的 N2 中 V0，使得 N2 中的数据 V0 也更新为 V1，N2 中的数据再响应 N2 的
请求。

这里，可以定义 N1 和 N2 的数据库 V 之间的数据是否一样为一致性；外部对 N1 和 N2 的请求响应为可用行；N1 和
N2 之间的网络环境为分区容错性。这是正常运作的场景，也是理想的场景，然而现实是残酷的，当错误发生的时候，一致性
和可用性还有分区容错性，是否能同时满足，还是说要进行取舍呢？

作为一个分布式系统，它和单机系统的最大区别，就在于网络，现在假设一种极端情况，N1 和 N2 之间的网络断开了，我
们要支持这种网络异常，相当于要满足分区容错性，能不能同时满足一致性和响应性呢？还是说要对他们进行取舍。

[[file:../../images/intro-CAP_Proof-of-CAP_03.png]]
假设在 N1 和 N2 之间网络断开的时候，有用户向 N1 发送数据更新请求，那 N1 中的数据 V0 将被更新为 V1，由于
网络是断开的，所以分布式系统同步操作 M，所以 N2 中的数据依旧是 V0；这个时候，有用户向 N2 发送数据读取请
求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据 V1，怎么办呢？

有二种选择，第一，牺牲数据一致性，保证可用性。响应旧的数据 V0 给用户；

第二，牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作 M 完成之后，再给用户响应最新的数据
V1。

这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。

* CAP 权衡
通过 CAP 理论及前面的证明，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？
我们分三种情况来阐述一下。

** CA without P
这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，
所以如果舍弃 P，意味着要舍弃分布式系统。那也就没有必要再讨论 CAP 理论了。这也是为什么在前面的 CAP 证明
中，我们以系统满足 P 为前提论述了无法同时满足 C 和 A。

比如我们熟知的关系型数据库，如 MySql 和 Oracle 就是保证了可用性和数据一致性，但是他并不是个分布式系统。
一旦关系型数据库要考虑主备同步、集群部署等就必须要把 P 也考虑进来。

其实，在 CAP 理论中。C、A、P三者并不是平等的，CAP 之父在《Spanner，真时，CAP 理论》一文中写到：

如果说 Spanner 真有什么特别之处，那就是谷歌的广域网。Google 通过建立私有网络以及强大的网络工程能力来
保证 P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。

从 Google 的经验中可以得到的结论是，无法通过降低 CA 来提升 P。要想提升系统的分区容错性，需要通过提升
基础设施的稳定性来保障。

所以，对于一个分布式系统来说。P是一个基本要求，CAP 三者中，只能在 CA 两者之间做权衡，并且要想尽办法提升
P。

** CP without A
如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在 CAP 三者中保障 CP 而
舍弃 A。

一个保证了 CP 而舍弃了 A 的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待
所有数据全部一致了之后再让用户访问系统。

设计成 CP 的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成 CP 的。在发生极端情况时，
优先保证数据的强一致性，代价就是舍弃系统的可用性。如 Redis、HBase 等，还有分布式系统中常用的
Zookeeper 也是在 CAP 三者之中选择优先保证 CP 的。

无论是像 Redis、HBase 这种分布式存储系统，还是像 Zookeeper 这种分布式协调组件。数据的一致性是他们最
基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？

在我的[[http://47.103.216.138/archives/1275][Zookeeper介绍（二）——Zookeeper概述]]一文中介绍过 zk 关于 CAP 的思考，这里再简单回顾一下：

ZooKeeper 是个 CP（一致性+分区容错性）的，即任何时刻对 ZooKeeper 的访问请求能得到一致的数据结果，
同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper 可能会
丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper 是分布式协调服务，它的职责是保证数据在其管
辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成 CP 而不是 AP 特性的了。

** AP wihtout C
要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户
访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。

这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系
统在可用性方面会做很多事情来保证系统的全年可用性可以达到 N 个 9，所以，对于很多业务系统来说，比如淘宝的
购物，12306 的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。

你在 12306 买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正
常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以
正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。

但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是
强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，
但是过了一段时间，还是要保证最终一致性的。

对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常
态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃 C（退而求其次保证最终一致性）。虽然某些地方会
影响客户体验，但没达到造成用户流程的严重阻塞程度。

** 适合的才是最好的
上面介绍了如何在 CAP 中权衡及取舍以及典型的案例。孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。

对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证 CP，舍弃 A。比如
前几年支付宝光缆被挖断的事件，在网络出现故障的时候，支付宝就在可用性和数据一致性之间选择了数据一致性，用
户感受到的是支付宝系统长时间宕机，但是其实背后是无数的工程师在恢复数据，保证数数据的一致性。

对于其他场景，比较普遍的做法是选择可用性和分区容错性，舍弃强一致性，退而求其次使用最终一致性来保证数据的
安全。这其实是分布式领域的另外一个理论——BASE 理论。我们下一篇文章再来介绍。

* 总结
无论你是一个架构师，还是一个普通开发，在设计或开发分布式系统的时候，不可避免的要在 CAP 中做权衡。需要根据
自己的系统的实际情况，选择最适合自己的方案。

* 参考资料：
- [[http://my.oschina.net/foodon/blog/372703][CAP和BASE理论]]
- [[http://dbaplus.cn/news-159-1917-1.html][一文带你重新审视CAP理论与分布式系统设计]]
- [[http://www.ruanyifeng.com/blog/2018/07/cap.html][CAP 定理的含义]]

